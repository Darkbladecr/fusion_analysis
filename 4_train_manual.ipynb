{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "Pulling manual data classification and placing it into train-validate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_chi = pd.read_csv('data/chi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi['selftext'] = df_chi['selftext'].fillna('')\n",
    "df_chi['text'] = df_chi['title'] + '\\n' + df_chi['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bool = ~df_chi.loc[:, 'negative'].str.startswith('0.')\n",
    "df_manual = df_chi.loc[manual_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tone</th>\n",
       "      <th>emotion</th>\n",
       "      <th>theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>neutral</td>\n",
       "      <td>fear</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tone emotion     theme\n",
       "count        26      26        26\n",
       "unique        3       5         5\n",
       "top     neutral    fear  question\n",
       "freq         16      19        16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.loc[:, ['tone', 'emotion', 'theme']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_labels = ['negative', 'neutral', 'positive']\n",
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "theme_labels = ['clinical update', 'community', 'question', 'education', 'advocating', 'dissuading', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_val_split(df, train_size, val_size, test_size, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "    - df: pandas dataframe to split.\n",
    "    - train_size: float between 0 and 1 indicating the proportion of the dataframe to include in the training set.\n",
    "    - val_size: float between 0 and 1 indicating the proportion of the dataframe to include in the validation set.\n",
    "    - test_size: float between 0 and 1 indicating the proportion of the dataframe to include in the test set.\n",
    "    - random_state: int or None, optional (default=42). The seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - train_df: pandas dataframe containing the training set.\n",
    "    - val_df: pandas dataframe containing the validation set.\n",
    "    - test_df: pandas dataframe containing the test set.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: if the sum of train_size, val_size, and test_size is not equal to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_size + val_size + test_size == 1, \"Train, validation, and test sizes must add up to 1.\"\n",
    "    \n",
    "    # Split the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the size of the validation set relative to the original dataframe\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    \n",
    "    # Split the training set into training and validation sets\n",
    "    train_df, val_df = train_test_split(train_df, test_size=val_ratio, random_state=random_state)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme']\n",
    "train_df, val_df, test_df = train_test_val_split(df_manual.loc[:, cols], 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_path_train = \"data/fine-tune/chi_train.jsonl\"\n",
    "manual_path_validate = \"data/fine-tune/chi_validate.jsonl\"\n",
    "manual_path_test = \"data/fine-tune/chi_test.jsonl\"\n",
    "\n",
    "train_df.to_json(manual_path_train, orient=\"records\", lines=True)\n",
    "val_df.to_json(manual_path_validate, orient=\"records\", lines=True)\n",
    "test_df.to_json(manual_path_test, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/stefan/.cache/huggingface/datasets/json/default-12827ee0da3e1b2b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451131c5af4a4d70bb27f38ea4e0e66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1f1f886cf84ee8b3140245a5f6a3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad698cb9b490407d875e2af419fa2c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dffe45171794fd5b15fe2698cfdf641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validate split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb21b31885534703b868ff67391826f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/stefan/.cache/huggingface/datasets/json/default-12827ee0da3e1b2b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801d2b69d68480497052f61ad39db09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': manual_path_train,\n",
    "    'validate': manual_path_validate,\n",
    "    'test': manual_path_test\n",
    "}\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13vs887',\n",
       " 'created_utc': '5/30/23 14:44',\n",
       " 'text': 'Ride go kart?\\nI had a spinal fusion done in 2010. My family and I went to the go kart track where they had an arcade and amusement rides. Year before last I drove a go kart and I remember I couldn’t really touch the gas pedal and bend because of how straight my back is, but everyone else around me could touch it just fine with their knees bent. It wasn’t that I was short, it was that I couldn’t bend. Has anyone else ever experienced this? Should you ride go karts if you’ve had this type of surgery?',\n",
       " 'tone': 'neutral',\n",
       " 'emotion': 'surprise',\n",
       " 'theme': 'question'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'contradiction', 1: 'neutral', 2: 'entailment'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-mnli\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['contradiction', 'neutral', 'entailment'], id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "class_labels = ClassLabel(num_classes=config.num_labels, names=list(config.id2label.values()))\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d253a1b16e4adaaff15748043433ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbb0f4bf807452ca13f20dde25cef67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456a4e4145584809b9c8f9666cbba54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680c0aedda5942b7acbcdb96db72b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1364b5ea3a469c9964dcbbad7010a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23c16e103b346eab615190b2efdb7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83ed8e61b5e40b7ab6506ad0f4364cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97a614f93dd4a908c8db7157c012833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b7bfd6b8fe4df88c7cda7973c1bf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label', 'input_sentence'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label', 'input_sentence'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label', 'input_sentence'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"True\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "\n",
    "theme_labels = ['clinical update', 'community', 'question', 'education', 'advocating', 'dissuading', 'other']\n",
    "num_labels = len(theme_labels)\n",
    "# class_labels = ClassLabel(num_classes=num_labels, names=theme_labels)\n",
    "template=\"This example is {}.\"\n",
    "\n",
    "def preprocess_func(row):\n",
    "    text = row['text']\n",
    "    themes = row['theme']\n",
    "    return {\n",
    "        'premise': text,\n",
    "        'hypothesis': [template.format(theme) for theme in themes]\n",
    "    }\n",
    "\n",
    "def encode_func(row):\n",
    "    premise = row['premise']\n",
    "    theme = row['theme'][0]\n",
    "    contradictions = [template.format(x) for x in theme_labels if x != theme]\n",
    "    encoded_input = tokenizer(\n",
    "        premise*num_labels, \n",
    "        [template.format(theme)] + contradictions,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    encoded_input['label'] = [2] + [0] * (num_labels - 1)\n",
    "    encoded_input['input_sentence'] = tokenizer.batch_decode(encoded_input.input_ids)\n",
    "\n",
    "    return encoded_input\n",
    "\n",
    "def isolate_dataset(ds: Dataset, feature: str):\n",
    "    cols = ds.column_names['train']\n",
    "    col_keep = {'text', feature}\n",
    "    \n",
    "    ds_filter = ds.remove_columns(col_keep.symmetric_difference(cols))\n",
    "    ds_filter = ds_filter.map(preprocess_func, batched=True, remove_columns=['text'])\n",
    "    ds_filter = ds_filter.map(encode_func, batched=True, batch_size=1, remove_columns=['theme', 'premise', 'hypothesis'])\n",
    "    ds_filter = ds_filter.cast_column('label', class_labels)\n",
    "\n",
    "    return ds_filter\n",
    "\n",
    "feature = \"theme\"\n",
    "ds_theme = isolate_dataset(ds, feature)\n",
    "ds_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'label': ClassLabel(names=['contradiction', 'neutral', 'entailment'], id=None),\n",
       " 'input_sentence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_theme['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_encoded = ds_theme.remove_columns([\"input_sentence\"])\n",
    "ds_encoded = ds_encoded.rename_column(\"label\", \"labels\")\n",
    "ds_encoded.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(ds_encoded['train'], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(ds_encoded['validate'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b4739fe2a4e11ab4e36464e80da28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.14285714285714285}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_analysis-xH46poxW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
